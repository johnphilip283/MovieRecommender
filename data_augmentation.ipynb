{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting The Wikipedia Plot Summary Dataset\n",
    "\n",
    "We start off the data collection and cleaning pipeline with two datasets from Wikipedia. In order to wrangle this data, I decided to use Python, more specificially, the CSV and Pandas modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import datetime\n",
    "import requests\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since a lot of the data was missing from the Wikipedia dataset, I decided to augment this dataset with more data from the OMDB API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL = \"http://www.omdbapi.com\"\n",
    "API_KEY = \"d8a2854c\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple utility function makes the request to the OMDB API and returns the data in JSON format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ping_api(movie_name, year=None, api_key=API_KEY):\n",
    "\n",
    "    request_params = {'apikey': api_key, 'type': 'movie', 't': movie_name, 'plot': 'full'}\n",
    "\n",
    "    if year:\n",
    "        request_params['y'] = year\n",
    "\n",
    "    r = requests.get(API_URL, params=request_params)\n",
    "\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I read in the dataframes and concatenate them into one big dataframe, so that we can iterate over it using the CSV module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the dataframes into one big dataset.\n",
    "df = pd.concat([pd.read_csv(\"datasets/wiki_dataset_1.csv\", index_col=0),\n",
    "                pd.read_csv(\"datasets/wiki_dataset_2.csv\", index_col=0)], ignore_index=True)\n",
    "\n",
    "# Create a CSV that I can use to read the data.\n",
    "df.to_csv(\"intermediate.csv\")\n",
    "\n",
    "imputation_targets = (\"Unknown\", \"N/A\", \"unknown\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Pipeline\n",
    "\n",
    "1. Open the concatenated dataframe, as well as the final CSV we will need to write to.\n",
    "2. Add the columns for the supplementary data.\n",
    "3. Query the API with the title of the movie found in each row, and if possible, the year it was produced.\n",
    "4. If the movie wasn't found on the database, continue with the next row.\n",
    "5. If we've reached the request limit on this API key, then sleep for 24 hours and continue the cycle.\n",
    "6. At this point, we have a valid JSON response from the API, so impute the fields with whatever it returns, and add the additional data to the appropriate columns.\n",
    "7. If there are any issues with any particular row, then continue with the next row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputes missing data from the wikipedia plot summary dataset with OMDB-scraped data, and supplements it with\n",
    "# extra data.\n",
    "with open(\"intermediate.csv\") as read_file, open(\"final.csv\", 'a') as write_file:\n",
    "\n",
    "    reader = csv.DictReader(read_file)\n",
    "\n",
    "    supplementary_data = [\"Runtime\", \"imdbRating\", \"imdbVotes\", \"Rated\", \"Rotten Tomatoes\", \"Metacritic\"]\n",
    "\n",
    "    writer = csv.DictWriter(write_file, fieldnames=df.columns.tolist() + supplementary_data)\n",
    "    writer.writeheader()\n",
    "    movie_json = {}\n",
    "\n",
    "    for row in reader:\n",
    "\n",
    "        row = dict(row)\n",
    "\n",
    "        try:\n",
    "            # Ping the API\n",
    "            movie_json = ping_api(row[\"Title\"],\n",
    "                                  year=row[\"Release Year\"] if row[\"Release Year\"] not in imputation_targets else None,\n",
    "                                  api_key=API_KEY)\n",
    "\n",
    "            # If the movie doesn't exist in the OMDB database, then skip this row, since we can't add any extra data\n",
    "            # to it.\n",
    "            if movie_json[\"Response\"] == \"False\":\n",
    "\n",
    "                # Repeat the queries\n",
    "                if movie_json[\"Error\"] == \"Request limit reached!\":\n",
    "                    print(\"Timed out: JSON response is: {}\".format(movie_json))\n",
    "\n",
    "                    sleep(3600 * 24)\n",
    "\n",
    "                elif movie_json[\"Error\"] == \"Movie not found!\":\n",
    "                    print(\"Not found: JSON response is: {}\".format(movie_json))\n",
    "\n",
    "                print(\"Empty API response for {}. \".format(row[\"Title\"]))\n",
    "\n",
    "                continue\n",
    "\n",
    "            print(\"{}:\\t\\tImputing data for {}.\".format(str(datetime.datetime.now()), row[\"Title\"]))\n",
    "\n",
    "            # If any of these fields are imputable, impute them.\n",
    "            if row[\"Director\"] in imputation_targets:\n",
    "                row[\"Director\"] = movie_json[\"Director\"] if movie_json[\"Director\"] != \"N/A\" else \"\"\n",
    "\n",
    "            if row[\"Release Year\"] in imputation_targets:\n",
    "                row[\"Release Year\"] = movie_json[\"Year\"]\n",
    "\n",
    "            if row[\"Genre\"] in imputation_targets:\n",
    "                row[\"Genre\"] = movie_json[\"Genre\"]\n",
    "\n",
    "            if row[\"Cast\"] in imputation_targets:\n",
    "                row[\"Cast\"] = movie_json[\"Actors\"] if movie_json[\"Actors\"] != \"N/A\" else \"\"\n",
    "\n",
    "            if row[\"Plot\"] in imputation_targets:\n",
    "                row[\"Plot\"] = movie_json[\"Plot\"]\n",
    "\n",
    "            # Supplement with additional data\n",
    "            row[\"imdbRating\"] = movie_json[\"imdbRating\"] if movie_json[\"imdbRating\"] != \"N/A\" else \"\"\n",
    "            row[\"imdbVotes\"] = movie_json[\"imdbVotes\"] if movie_json[\"imdbVotes\"] != \"N/A\" else \"\"\n",
    "            row[\"Rated\"] = movie_json[\"Rated\"] if movie_json[\"Rated\"] not in (\"N/A\", \"NOT RATED\", \"PASSED\", \"UNRATED\", \"APPROVED\") else \"\"\n",
    "\n",
    "            # Go through the ratings list, and add the appropriate fields.\n",
    "            if \"Ratings\" in movie_json:\n",
    "                for rating in movie_json[\"Ratings\"]:\n",
    "                    row[rating[\"Source\"]] = rating[\"Value\"] if rating[\"Source\"] in supplementary_data else \"\"\n",
    "\n",
    "            writer.writerow({key: val for key, val in row.items() if key != \"\"})\n",
    "\n",
    "        except KeyError as e:\n",
    "\n",
    "            print(\"Movie JSON: {}\".format(movie_json))\n",
    "            print(\"Row: {}\".format(row) + \"\\n\")\n",
    "            print(\"{}:\\t\\tRan into a key error for row {}.\".format(str(datetime.datetime.now()), row[\"Title\"]))\n",
    "\n",
    "        except (KeyboardInterrupt, Exception) as e:\n",
    "            raise e\n",
    "            \n",
    "            print(\"Writing files ...\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After imputing the missing data, and augmenting the dataset with the Rotten Tomatoes and Metacritic data, write it out to a CSV file, and split it back up. I split up the data because my computer doesn't allow me to open incredibly large files, so I look at each one separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final.csv\", index_col=0)\n",
    "\n",
    "# Write out the new dataframe to two CSVs.\n",
    "df[:int(len(df) / 2)].to_csv(\"souped_up_movie_dataset_1.csv\")\n",
    "\n",
    "df[int(len(df) / 2):].to_csv(\"souped_up_movie_dataset_2.csv\")\n",
    "\n",
    "if os.path.isfile(\"intermediate.csv\"):\n",
    "    os.remove(\"intermediate.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
